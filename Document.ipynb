{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7880ff7c",
   "metadata": {},
   "source": [
    "# **Minekaa RL Parkour**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e872ad4",
   "metadata": {},
   "source": [
    "Member :\n",
    "- Napat Aeimwiratchai 65340500020\n",
    "- Phattarawat kadrum 65340500074"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2fca9c",
   "metadata": {},
   "source": [
    "Goal : Train RL agent to play a Minekaa parkour map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b89779e",
   "metadata": {},
   "source": [
    "## What is Minekaa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab6548",
   "metadata": {},
   "source": [
    "Minekaa is a 3D Python game that replicates Minecraft, but simplified to include only the important character actions used for parkour gameplay. These include movement (forward, backward, left, right, jump) and camera control (look up, look down, rotate left, rotate right)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df73c56",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"asset/minekaa.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55de996",
   "metadata": {},
   "source": [
    "## Previous work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faa4ed9",
   "metadata": {},
   "source": [
    "### RLMinecraftParkour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20fde23",
   "metadata": {},
   "source": [
    "from https://github.com/LouisCaubet/RLMinecraftParkour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1920667",
   "metadata": {},
   "source": [
    "Use PPO to trained agent to play minecraft parkour\n",
    "- Level\n",
    "    - Level 1: Straight line\n",
    "    - Level 2: Narrower straight line with one-block jump\n",
    "    \n",
    "- Rewards:\n",
    "    - +100 for reaching the diamond block (Mian task)\n",
    "    - +10 for each (gold) block towards the goal (Mini reward for get closer to goal)\n",
    "    - -100 and end of episode when touching the bedrock (Penalty term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc9dd0",
   "metadata": {},
   "source": [
    "### MineRl_parkour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5d410",
   "metadata": {},
   "source": [
    "from https://github.com/seantey/minerl_parkour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7971af",
   "metadata": {},
   "source": [
    "- Environment: MineRL + Microsoft Malmo (Minecraft interface) + OpenAI Gym\n",
    "- Agent: Minecraft Player Bot\n",
    "- States: Observable state is pixels of player first person P.O.V, full state is the entire minecraft world.\n",
    "- Actions: First Person Shooter controls (up,down,left,right,strafe,camera)\n",
    "- Completed Level 1: Straight Line Cliffwalking Lava Map\n",
    "- Rewards:\n",
    "    - +100 if player reaches diamon block goal\n",
    "    - +10 for every block distance closer to goal\n",
    "    - -100 for death / drowining in lava"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e14c1",
   "metadata": {},
   "source": [
    "## **Experiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d1ccfe",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"asset/Axis.png\" alt=\"Alt text\" width=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5413b8",
   "metadata": {},
   "source": [
    "### Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b5c867",
   "metadata": {},
   "source": [
    "- Compare Performance of DQN and SAC (SAC-Discrete) in Parkour task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c825503b",
   "metadata": {},
   "source": [
    "- Adding reward & action term to improve model performance. (Add more reward & action compare to ref. paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d9cb89",
   "metadata": {},
   "source": [
    "### **Action & Reward**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638c486",
   "metadata": {},
   "source": [
    "**Action** (10 actions): \n",
    "- Forward (1)\n",
    "- Backward (1)\n",
    "- Slide Left/Right (2)\n",
    "- Rotate(Roll) 0/45/90 degree (3)\n",
    "- Rotate(Pitch) Left/Right (2)\n",
    "- Jump (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4983225d",
   "metadata": {},
   "source": [
    "**Reward:**\n",
    "- +100: when agent reach goal\n",
    "- -50: hit lava \n",
    "- Mini reward: agent closer to goal (Euclidian)\n",
    "- Penalty: increase based on how long it takes step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40fc5be",
   "metadata": {},
   "source": [
    "### **Parkour map**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c155fc88",
   "metadata": {},
   "source": [
    "**Level 1:** Straight Forward map  \n",
    "For test the setup of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02de106e",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"asset/straightmap.png\" alt=\"Alt text\" width=\"200\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea6110d",
   "metadata": {},
   "source": [
    "**Level 2:** Zigzag path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ff5338",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"asset/zigzagmap.png\" alt=\"Alt text\" width=\"200\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd53a15",
   "metadata": {},
   "source": [
    "### **Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2aafe2",
   "metadata": {},
   "source": [
    "#### DQN (Deep Q-Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bc33f2",
   "metadata": {},
   "source": [
    "- Q Learning\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s,a) \\right]\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d28ffc",
   "metadata": {},
   "source": [
    "- In this task where obs is pixel of image using the Q-table to store Q-values is hard\n",
    "- So DQN use neural network to approximate Q-function $Q(s, a; \\theta)$\n",
    "- Stores the agent’s experiences $(s, a, r, s’)$ in a replay buffer and samples mini-batches randomly to break correlation between samples and stabilize training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f105bfd7",
   "metadata": {},
   "source": [
    "and want to minimize the loss function while training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac39e590",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{loss} = \\left( r + \\gamma \\max_{a'} \\hat{Q}(s', a') - Q(s, a) \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f340a",
   "metadata": {},
   "source": [
    "#### SAC (Soft Actor critic-Discrete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126de5d",
   "metadata": {},
   "source": [
    "We use modified SAC that work with discrete action. Refference from https://github.com/toshikwa/sac-discrete.pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afeb91c",
   "metadata": {},
   "source": [
    "Soft Actor-Critic wants to find a policy that maximises the maximum entropy objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6508917",
   "metadata": {},
   "source": [
    "$$\\pi^* = \\arg\\max_{\\pi} \\sum_{t=0}^{T} \\mathbb{E}_{(s_t, a_t) \\sim \\tau_\\pi} \\left[ \\gamma^t \\left( r(s_t, a_t) + \\alpha \\mathcal{H}(\\pi(\\cdot | s_t)) \\right) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f565700",
   "metadata": {},
   "source": [
    "Where Entropy-Regularized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a728ad99",
   "metadata": {},
   "source": [
    "$$\\mathcal{H}(\\pi(\\cdot | s)) = - \\mathbb{E}_{a \\sim \\pi(\\cdot | s)} \\left[ \\log \\pi(a | s) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1944ddec",
   "metadata": {},
   "source": [
    "Entropy measures how random a random variable is. A high entropy means more exploration. SAC want to maximize entropy so that the agent can explore a wide range of actions, increasing the chances of finding the best policy through better actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff74a4",
   "metadata": {},
   "source": [
    "reward objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876c7ba2",
   "metadata": {},
   "source": [
    "$$J(\\pi) = \\mathbb{E}_{(s_t, a_t) \\sim \\rho_\\pi} \\left[ r(s_t, a_t) + \\alpha \\mathcal{H}(\\pi(\\cdot | s_t)) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe17c470",
   "metadata": {},
   "source": [
    "SAC is state of the art sample efficiency in multiple challenging continuous control\n",
    "domains. The paper purpose SAC-D that modified the SAC from continuous action into discrete action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4cacbb",
   "metadata": {},
   "source": [
    "- first thing modified $\\pi_\\phi \\left( a_t \\mid s_t \\right)$ to outputs a probability instead of a density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0109e7",
   "metadata": {},
   "source": [
    "$V(s_t) := \\mathbb{E}_{a_t \\sim \\pi} \\left[ Q(s_t, a_t) - \\alpha \\log \\left( \\pi(a_t \\mid s_t) \\right) \\right]$ `to` $V(s_t) := \\pi(s_t)^T \\left[ Q(s_t) - \\alpha \\log\\left( \\pi(s_t) \\right) \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9619a67",
   "metadata": {},
   "source": [
    "- Change calculation of the temperature loss to reduce the variance of that estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b846eb6e",
   "metadata": {},
   "source": [
    "$J(\\alpha) = \\mathbb{E}_{a_t \\sim \\pi_t} \\left[ -\\alpha \\left( \\log \\pi_t(a_t \\mid s_t) + \\bar{H} \\right) \\right]$ `to` $J(\\alpha) = \\pi_t(s_t)^T \\left[ -\\alpha \\left( \\log(\\pi_t(s_t)) + \\bar{H} \\right) \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4caeca",
   "metadata": {},
   "source": [
    "- Policy objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cef757",
   "metadata": {},
   "source": [
    "$J_{\\pi}(\\phi) = \\mathbb{E}_{s_t \\sim D, \\epsilon_t \\sim \\mathcal{N}} \\left[ \\alpha \\log\\left( \\pi_{\\phi}(f_{\\phi}(\\epsilon_t; s_t) \\mid s_t) \\right) - Q_{\\theta}(s_t, f_{\\phi}(\\epsilon_t; s_t)) \\right]$ `to` $J_{\\pi}(\\phi) = \\mathbb{E}_{s_t \\sim D} \\left[ \\pi_t(s_t)^T \\left[ \\alpha \\log(\\pi_{\\phi}(s_t)) - Q_{\\theta}(s_t) \\right] \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbce972",
   "metadata": {},
   "source": [
    "### **Reinforcement learning component**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23662954",
   "metadata": {},
   "source": [
    "#### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11526fb3",
   "metadata": {},
   "source": [
    "Custom map env made from ursina + stable_baseline 3 for algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7645e5d5",
   "metadata": {},
   "source": [
    "#### State "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb467f87",
   "metadata": {},
   "source": [
    "Observation is pixel-image from agent FPV (first person view) and agent position (use to compute reward)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac7f93",
   "metadata": {},
   "source": [
    "#### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef18d0",
   "metadata": {},
   "source": [
    "```py\n",
    "# 0 move forward\n",
    "self.player.position += forward * speed\n",
    "# 1 move backward\n",
    "self.player.position -= forward * speed\n",
    "# 2 left\n",
    "self.player.position -= right * speed\n",
    "# 3 right\n",
    "self.player.position += right * speed\n",
    "# 4 look down 45 degrees\n",
    "self.player.rotation_x = 45\n",
    "# 5 look down 90 degrees\n",
    "self.player.rotation_x = 90\n",
    "# 6 look forward (reset pitch)\n",
    "self.player.rotation_x = 0\n",
    "# 7 rotate left\n",
    "self.current_angle_index = (self.current_angle_index - 1) % 4\n",
    "self.player.rotation_y = self.rotation_angles[self.current_angle_index]\n",
    "# 8 rotate right\n",
    "self.current_angle_index = (self.current_angle_index + 1) % 4\n",
    "elf.player.rotation_y = self.rotation_angles[self.current_angle_index]\n",
    "# 9 jump forward\n",
    "self.player.velocity.y = self.jump_force\n",
    "self.player.position += forward * 0.2\n",
    "self.on_ground = False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8716b9",
   "metadata": {},
   "source": [
    "### **Result**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e2f39",
   "metadata": {},
   "source": [
    "#### **First map (straight)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ede40",
   "metadata": {},
   "source": [
    "**DQN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d2be7",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center; align-items: center; gap: 20px;\">\n",
    "    <img src=\"asset/DQNstvis.png\" alt=\"Zigzag Map\" width=\"200\" />\n",
    "    <video width=\"500\" controls>\n",
    "        <source src=\"asset/DQN_straight.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4ca9a",
   "metadata": {},
   "source": [
    "- The agent can reach the goal, but when it touches the goal, it chooses to move backward. This may be due to a random action that happens to yield the highest reward, which the agent think it as a optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d26d9f",
   "metadata": {},
   "source": [
    "If you can't see the video can you can view at asset/DQN_straight.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90582967",
   "metadata": {},
   "source": [
    "**SAC**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5ca783",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center; align-items: center; gap: 20px;\">\n",
    "    <img src=\"asset/SACstvis.png\" alt=\"Zigzag Map\" width=\"200\" />\n",
    "    <video width=\"500\" controls>\n",
    "        <source src=\"asset/sac_evalst.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1198fb7d",
   "metadata": {},
   "source": [
    "- The agent can receive a positive reward when taking steps closer to the goal, but there seem to be many random actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f30bd5",
   "metadata": {},
   "source": [
    "If you can't see the video can you can view at asset/sac_evalst.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f6833d",
   "metadata": {},
   "source": [
    "#### **Second map (zigzag)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a676e2",
   "metadata": {},
   "source": [
    "**DQN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe7a75",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center; align-items: center; gap: 20px;\">\n",
    "    <img src=\"asset/DQNzigvis.png\" alt=\"Zigzag Map\" width=\"200\" />\n",
    "    <video width=\"500\" controls>\n",
    "        <source src=\"asset/DQN_zigzag.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadedc4d",
   "metadata": {},
   "source": [
    "- The agent is able to move about halfway to the goal, but afterward, it repeatedly chooses the same action (looking down (action 4)) until it gets terminated due to reaching the maximum timestep. It’s possible that the agent prefers to incur a penalty from repeating actions rather than falling into the lava."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0f34d3",
   "metadata": {},
   "source": [
    "If you can't see the video can you can view at asset/DQN_zigzag.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e974dce",
   "metadata": {},
   "source": [
    "**SAC**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4f2647",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center; align-items: center; gap: 20px;\">\n",
    "    <img src=\"asset/SACzigvis.png\" alt=\"Zigzag Map\" width=\"200\" />\n",
    "    <video width=\"500\" controls>\n",
    "        <source src=\"asset/sac_evalzig.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d10e26",
   "metadata": {},
   "source": [
    "- The agent can move a little, but it still seems to be taking random actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee04c5b",
   "metadata": {},
   "source": [
    "If you can't see the video can you can view at asset/sac_evalzig.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3196bd88",
   "metadata": {},
   "source": [
    "### Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9062a65",
   "metadata": {},
   "source": [
    "Adding reward term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91098855",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"asset/compare_penalty.png\" alt=\"Alt text\" width=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66261a55",
   "metadata": {},
   "source": [
    "Before adding the reward term (penalty: increases with step duration), we see that the agent sometimes repeated actions or performed actions that did not contribute to increasing the reward (such as rotating in place). However, after we add the reward term (a penalty that increases with the number of steps taken), we can saw that the training reward improved, even when using the same hyperparameters and a fixed random seed. The agent also tended to perform more meaningful actions, with fewer unnecessary behaviors. But, in some environments, the agent still takes an unimportant actions (repeatedly adjusting the camera view) until the episode ends due to reaching the maximum timestep. This might be because the penalty for taking additional actions reduces the total reward less than the penalty incurred for falling into lava."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7f2d06",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center; align-items: center; gap: 20px;\">\n",
    "    <img src=\"asset/SACDgraph.png\" alt=\"Zigzag Map\" width=\"500\" />\n",
    "    <img src=\"asset/SACDsmoothgraph.png\" alt=\"Zigzag Map\" width=\"500\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c738e144",
   "metadata": {},
   "source": [
    "Problem with SAC-D: Reward is converge to 0\n",
    "- Entropy converge too fast\n",
    "- Under exploration\n",
    "- too late update target_network_frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0b7ec",
   "metadata": {},
   "source": [
    "In conclusion both algorithms seem to be able to increase the reward during training, but they still require tuning of hyperparameters and the number of training steps to achieve better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c24ecc2",
   "metadata": {},
   "source": [
    "### Future Improve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e949073",
   "metadata": {},
   "source": [
    "- Add penalty for timeout (terminate)\n",
    "- Make Continue action and train with Continuous SAC\n",
    "- Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d939e2e",
   "metadata": {},
   "source": [
    "**Reference**  \n",
    "[1] https://doi.org/10.48550/arXiv.1910.07207  \n",
    "[2] https://github.com/LouisCaubet/RLMinecraftParkour  \n",
    "[3] https://github.com/seantey/minerl_parkour"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
